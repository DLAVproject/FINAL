{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1a2_C8DQiSSn",
      "metadata": {
        "id": "1a2_C8DQiSSn"
      },
      "source": [
        "Brief Explanation of the Approach Used in Milestone 1:\n",
        "\n",
        "Main Approach:\n",
        "\n",
        "Our approach is a top-down approach. We first start by detecting all people in the frame, using the 'MobileNet SSD 512' model, under the pretense that the model is edge-device friendly.\n",
        "\n",
        "Each detected bounding box is then fed as input to a Pose-Estimator Model. We use a 'SimplePose ResNet18' Model for this, again favoring a trade-off between inference speed and accuracy. \n",
        "\n",
        "For each estimated skeleton, we calculate the angle between specific keypoints (joints). this allows us to get right_elbow_angle and the right_shoulder_angle. if both angles fall within a certain range, we classify the pose as a 'Power to the People' pose, and a bounding box with a label 'triggered' is rendered around that person.\n",
        "\n",
        "Another tested pose is the 'T' pose, which we also coded.\n",
        "\n",
        "Alternative Approach:\n",
        "\n",
        "An alternative approach that we tested out was to feed the estimated pose (skeleton) as a flattened vector to an 'Action Classifier' model. The model, with its weights download from '', was trained on 4,000 images containing 5 different classes/actions.\n",
        "\n",
        "From the 5 classes, we chose 'waving' as our trigger class. Consequently, the skeleton for which the classifier detects as 'waving' is targeted and a resulting bounding box, labeled 'triggered' is rendered around it.\n",
        "\n",
        "We went with the first approach due to its robustness against environment variables and human-skeleton length, esoecially that 4,000 datapoints for 5 classes, 900 of which were for our 'trigger' class, are not relatively a lot.\n",
        "\n",
        "Below we present the code for both approaches, as both were presented as working and feasible soutions for mileston 1. Ho"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2VfLpfcOhJ3P",
      "metadata": {
        "id": "2VfLpfcOhJ3P"
      },
      "outputs": [],
      "source": [
        "# Installing Requisites\n",
        "#!pip install mxnet gluoncv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddf5df1f",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-04-27T14:33:27.850713Z",
          "start_time": "2022-04-27T14:33:22.826849Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddf5df1f",
        "outputId": "d01ed1ae-deda-4904-bca3-691fc624f2c0"
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n",
        "\n",
        "# Import tensorflow only if running 'Alternative Approach'\n",
        "#import tensorflow\n",
        "#print(tensorflow.__version__)\n",
        "\n",
        "import argparse, time, logging, os, math, tqdm, cv2\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import mxnet as mx\n",
        "from mxnet import gluon, nd, image\n",
        "from mxnet.gluon.data.vision import transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gluoncv as gcv\n",
        "from gluoncv import data\n",
        "from gluoncv.data import mscoco\n",
        "from gluoncv.model_zoo import get_model\n",
        "from gluoncv.data.transforms.pose import detector_to_simple_pose, heatmap_to_coord\n",
        "from gluoncv.utils.viz import cv_plot_image, cv_plot_keypoints, plot_image\n",
        "\n",
        "from gluoncv import model_zoo, data, utils\n",
        "from gluoncv.data.transforms.pose import detector_to_alpha_pose, heatmap_to_coord_alpha_pose\n",
        "\n",
        "import heapq"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "894b3d59",
      "metadata": {
        "id": "894b3d59"
      },
      "source": [
        "Helper Functions - Action Classification - **Neural Net Approach (Alternative Approach)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c897f6d2",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-04-27T14:33:27.881814Z",
          "start_time": "2022-04-27T14:33:27.850713Z"
        },
        "id": "c897f6d2"
      },
      "outputs": [],
      "source": [
        "def pose_normalization(x):\n",
        "    '''\n",
        "    Normalizes the output vector of the pose-estimation model, which will be used\n",
        "    as input to the action-classification model. \n",
        "    '''\n",
        "    def retrain_only_body_joints(x_input):\n",
        "        x0 = x_input.copy()\n",
        "        x0 = x0[2:2+13*2] # disregards face-related points\n",
        "        return x0\n",
        "\n",
        "    def normalize(x_input):\n",
        "        # Separate original data into x_list and y_list\n",
        "        lx = []\n",
        "        ly = []\n",
        "        N = len(x_input)\n",
        "        i = 0\n",
        "        while i<N:\n",
        "            lx.append(x_input[i])\n",
        "            ly.append(x_input[i+1])\n",
        "            i+=2\n",
        "        lx = np.array(lx)\n",
        "        ly = np.array(ly)\n",
        "\n",
        "        # Get rid of undetected data (=0)\n",
        "        non_zero_x = []\n",
        "        non_zero_y = []\n",
        "        for i in range(int(N/2)):\n",
        "            if lx[i] != 0:\n",
        "                non_zero_x.append(lx[i])\n",
        "            if ly[i] != 0:\n",
        "                non_zero_y.append(ly[i])\n",
        "        if len(non_zero_x) == 0 or len(non_zero_y) == 0:\n",
        "            return np.array([0] * N)\n",
        "\n",
        "        # Normalization x/y data according to the bounding box\n",
        "        origin_x = np.min(non_zero_x)\n",
        "        origin_y = np.min(non_zero_y)\n",
        "        len_x = np.max(non_zero_x) - np.min(non_zero_x)\n",
        "        len_y = np.max(non_zero_y) - np.min(non_zero_y)\n",
        "        x_new = []\n",
        "        for i in range(int(N/2)):\n",
        "            if (lx[i] + ly[i]) == 0:\n",
        "                x_new.append(-1)\n",
        "                x_new.append(-1)\n",
        "            else:\n",
        "                x_new.append((lx[i] - origin_x) / len_x)\n",
        "                x_new.append((ly[i] - origin_y) / len_y)\n",
        "        return x_new\n",
        "\n",
        "    x_body_joints_xy = retrain_only_body_joints(x)\n",
        "    x_body_joints_xy = normalize(x_body_joints_xy)\n",
        "    return x_body_joints_xy\n",
        "\n",
        "def drawActionResult(img_display, skeleton, str_action_type):\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX \n",
        "\n",
        "    minx = 999\n",
        "    miny = 999\n",
        "    maxx = -999\n",
        "    maxy = -999\n",
        "    i = 0\n",
        "    NaN = 0\n",
        "\n",
        "    while i < len(skeleton):\n",
        "        if not(skeleton[i]==NaN or skeleton[i+1]==NaN):\n",
        "            minx = min(minx, skeleton[i])\n",
        "            maxx = max(maxx, skeleton[i])\n",
        "            miny = min(miny, skeleton[i+1])\n",
        "            maxy = max(maxy, skeleton[i+1])\n",
        "        i+=2\n",
        "\n",
        "    minx = int(minx * img_display.shape[1])\n",
        "    miny = int(miny * img_display.shape[0])\n",
        "    maxx = int(maxx * img_display.shape[1])\n",
        "    maxy = int(maxy * img_display.shape[0])\n",
        "    print(minx, miny, maxx, maxy)\n",
        "    \n",
        "    # Draw bounding box\n",
        "    # drawBoxToImage(img_display, [minx, miny], [maxx, maxy])\n",
        "    img_display = cv2.rectangle(img_display,(minx, miny),(maxx, maxy),(0,255,0), 4)\n",
        "\n",
        "    # Draw text at left corner\n",
        "\n",
        "\n",
        "    box_scale = max(0.5, min(2.0, (1.0*(maxx - minx)/img_display.shape[1] / (0.3))**(0.5) ))\n",
        "    fontsize = 1.5 * box_scale\n",
        "    linewidth = int(math.ceil(3 * box_scale))\n",
        "\n",
        "    TEST_COL = int( minx + 5 * box_scale)\n",
        "    TEST_ROW = int( miny - 10 * box_scale)\n",
        "\n",
        "    img_display = cv2.putText(img_display, str_action_type, (TEST_COL, TEST_ROW), font, fontsize, (0, 0, 255), linewidth, cv2.LINE_AA)\n",
        "\n",
        "    return img_display\n",
        "\n",
        "def match_pose_output_to_classifier_input(pred_coords):\n",
        "    '''\n",
        "    Re-orders the elements of our pose-estimator output vector into the\n",
        "    keypoints order of the input vector the action classifier model was trained\n",
        "    on.\n",
        "    '''\n",
        "\n",
        "    alphapose_resnet_joint_odering_dict = {\"nose\":0, \n",
        "                                        \"left_eye\":0,\n",
        "                                        \"right_eye\":0,\n",
        "                                        \"left_ear\":0,\n",
        "                                        \"right_ear\":0,\n",
        "                                        \"left_shoulder\":0,\n",
        "                                        \"right_shoulder\":0,\n",
        "                                        \"left_elbow\":0,\n",
        "                                        \"right_elbow\":0,\n",
        "                                        \"left_wrist\":0,\n",
        "                                        \"right_wrist\":0,\n",
        "                                        \"left_hip\":0,\n",
        "                                        \"right_hip\":0,\n",
        "                                        \"left_knee\":0,\n",
        "                                        \"right_knee\":0,\n",
        "                                        \"left_ankle\":0,\n",
        "                                        \"right_ankle\":0}  \n",
        "\n",
        "    tf_pose_est_joint_odering_dict =      {\"nose\":0, \n",
        "                                        \"neck\":0,\n",
        "                                        \"right_shoulder\":0,\n",
        "                                        \"right_elbow\":0,\n",
        "                                        \"right_wrist\":0,\n",
        "                                        \"left_shoulder\":0,\n",
        "                                        \"left_elbow\":0,\n",
        "                                        \"left_wrist\":0,\n",
        "                                        \"right_hip\":0,\n",
        "                                        \"right_knee\":0,\n",
        "                                        \"right_ankle\":0,\n",
        "                                        \"left_hip\":0,\n",
        "                                        \"left_knee\":0,\n",
        "                                        \"left_ankle\":0,\n",
        "                                        \"right_eye\":0,\n",
        "                                        \"left_eye\":0,\n",
        "                                        \"right_ear\":0,\n",
        "                                        \"left_ear\":0}  \n",
        "\n",
        "    for i,key in enumerate(alphapose_resnet_joint_odering_dict):\n",
        "        alphapose_resnet_joint_odering_dict[key] = pred_coords[i].asnumpy()\n",
        "    alphapose_resnet_joint_odering_dict[\"neck\"] = 0.5*(alphapose_resnet_joint_odering_dict[\"left_shoulder\"] + alphapose_resnet_joint_odering_dict[\"right_shoulder\"])\n",
        "    for key in tf_pose_est_joint_odering_dict.keys():\n",
        "        tf_pose_est_joint_odering_dict[key] = alphapose_resnet_joint_odering_dict[key]\n",
        "\n",
        "    skeleton_classifier_input = []\n",
        "\n",
        "    for value in tf_pose_est_joint_odering_dict.values():\n",
        "        skeleton_classifier_input.append(value[0])\n",
        "        skeleton_classifier_input.append(value[1])\n",
        "        \n",
        "    skeleton_classifier_input = np.array(skeleton_classifier_input)\n",
        "\n",
        "    return skeleton_classifier_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab063f83",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-04-27T14:33:27.897817Z",
          "start_time": "2022-04-27T14:33:27.883814Z"
        },
        "id": "ab063f83"
      },
      "outputs": [],
      "source": [
        "class ActionClassifier(object):\n",
        "    '''\n",
        "    A wrapper class for our action classifier\n",
        "    '''\n",
        "    def __init__(self, model_path):\n",
        "        from keras.models import load_model\n",
        "\n",
        "        self.dnn_model = load_model(model_path)\n",
        "        self.action_dict = [\"kick\", \"punch\", \"squat\", \"stand\", \"wave\"]\n",
        "\n",
        "    def predict(self, skeleton):\n",
        "\n",
        "        # Preprocess data\n",
        "        tmp = pose_normalization(skeleton)\n",
        "        skeleton_input = np.array(tmp).reshape(-1, len(tmp))\n",
        "            \n",
        "        # Predicted label: int & string\n",
        "        predicted_idx = np.argmax(self.dnn_model.predict(skeleton_input))\n",
        "        predicted_label = self.action_dict[predicted_idx]\n",
        "\n",
        "        return predicted_label"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c7e0049",
      "metadata": {
        "id": "6c7e0049"
      },
      "source": [
        "Helper Functions - Action Classification - **Angle Calculation Approach (Main Approach)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3232766",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-04-27T14:33:27.913821Z",
          "start_time": "2022-04-27T14:33:27.899818Z"
        },
        "id": "e3232766"
      },
      "outputs": [],
      "source": [
        "def calculateAngle(landmark1, landmark2, landmark3):\n",
        "    '''\n",
        "    This function calculates angle between three different landmarks.\n",
        "    Args:\n",
        "        landmark1: The first landmark containing the x,y and z coordinates.\n",
        "        landmark2: The second landmark containing the x,y and z coordinates.\n",
        "        landmark3: The third landmark containing the x,y and z coordinates.\n",
        "    Returns:\n",
        "        angle: The calculated angle between the three landmarks.\n",
        "\n",
        "    '''\n",
        "\n",
        "    # Get the required landmarks coordinates.\n",
        "    x1, y1 = landmark1\n",
        "    x2, y2 = landmark2\n",
        "    x3, y3 = landmark3\n",
        "\n",
        "    # Calculate the angle between the three points\n",
        "    angle = math.degrees(math.atan2(y3 - y2, x3 - x2) - math.atan2(y1 - y2, x1 - x2))\n",
        "    angle = np.abs(angle)\n",
        "    # Check if the angle is less than zero.\n",
        "    if angle > 180.0:\n",
        "\n",
        "        angle = 360-angle\n",
        "    \n",
        "    # Return the calculated angle.\n",
        "    return angle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "513f2959",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-04-27T14:33:27.929825Z",
          "start_time": "2022-04-27T14:33:27.915822Z"
        },
        "id": "513f2959"
      },
      "outputs": [],
      "source": [
        "def classifyPose(kp_array, output_image=None, display=False):\n",
        "    '''\n",
        "    This function classifies yoga poses depending upon the angles of various body joints.\n",
        "    Args:\n",
        "        kp_array: A list of detected landmarks of the person whose pose needs to be classified.\n",
        "        output_image: A image of the person with the detected pose landmarks drawn.\n",
        "        display: A boolean value that is if set to true the function displays the resultant image with the pose label \n",
        "        written on it and returns nothing.\n",
        "    Returns:\n",
        "        output_image: The image with the detected pose landmarks drawn and pose label written.\n",
        "        label: The classified pose label of the person in the output_image.\n",
        "\n",
        "    '''\n",
        "    \n",
        "    # Initialize the label of the pose. It is not known at this stage.\n",
        "    label = 'Unknown Pose'\n",
        "\n",
        "    # Specify the color (Red) with which the label will be written on the image.\n",
        "    color = (0, 0, 255)\n",
        "    \n",
        "    # Calculate the required angles.\n",
        "    #----------------------------------------------------------------------------------------------------------------\n",
        "    \n",
        "    # Get the angle between the left shoulder, elbow and wrist points. \n",
        "    left_elbow_angle = calculateAngle(kp_array[5],\n",
        "                                      kp_array[7],\n",
        "                                      kp_array[9])\n",
        "    \n",
        "    # Get the angle between the right shoulder, elbow and wrist points.\n",
        "    right_elbow_angle = calculateAngle(kp_array[6],\n",
        "                                       kp_array[8],\n",
        "                                       kp_array[10])\n",
        "    \n",
        "    # Get the angle between the left elbow, shoulder and hip points.\n",
        "    left_shoulder_angle = calculateAngle(kp_array[7],\n",
        "                                         kp_array[5],\n",
        "                                         kp_array[11])\n",
        "    # Get the angle between the right hip, shoulder and elbow points.\n",
        "    right_shoulder_angle = calculateAngle(kp_array[12],\n",
        "                                          kp_array[6],\n",
        "                                          kp_array[8])\n",
        "    \n",
        "    #print('left_elbow_angle: ', left_elbow_angle ,'\\n right_elbow_angle: ', right_elbow_angle)\n",
        "    #print('left_shoulder_angle: ', left_shoulder_angle ,'\\n right_shoulder_angle: ', right_shoulder_angle)\n",
        "    \n",
        "    #----------------------------------------------------------------------------------------------------------------\n",
        "    \n",
        "    # Check if it is the warrior II pose or the T pose.\n",
        "    # As for both of them, both arms should be straight and shoulders should be at the specific angle.\n",
        "    #----------------------------------------------------------------------------------------------------------------\n",
        "    \n",
        "    # Check if the both arms are straight.\n",
        "    if left_elbow_angle > 125 and left_elbow_angle < 220 and right_elbow_angle > 125 and right_elbow_angle < 220:\n",
        "        #label = 'T Pose'\n",
        "        # Check if shoulders are at the required angle.\n",
        "        if left_shoulder_angle > 70 and left_shoulder_angle < 110 and right_shoulder_angle > 70 and right_shoulder_angle < 110:\n",
        "            label = 'T Pose'\n",
        "\n",
        "    if right_elbow_angle > 50 and right_elbow_angle < 130 and right_shoulder_angle > 70 and right_shoulder_angle < 110:\n",
        "        label = 'Power to the People'                  \n",
        "    \n",
        "    # Check if the pose is classified successfully\n",
        "    if label != 'Unknown Pose':\n",
        "        \n",
        "        # Update the color (to green) with which the label will be written on the image.\n",
        "        color = (0, 255, 0)  \n",
        "    \n",
        "    # Write the label on the output image. \n",
        "    #cv2.putText(output_image, label, (10, 30),cv2.FONT_HERSHEY_PLAIN, 1, color, 2)\n",
        "    \n",
        "    # Check if the resultant image is specified to be displayed.\n",
        "    if display:\n",
        "    \n",
        "        # Display the resultant image.\n",
        "        plt.figure(figsize=[10,10])\n",
        "        plt.imshow(output_image[:,:,::-1]);plt.title(\"Output Image\");plt.axis('off');\n",
        "        \n",
        "    else:\n",
        "        \n",
        "        # Return the output image and the classified label.\n",
        "        return label"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "936249a0",
      "metadata": {
        "id": "936249a0"
      },
      "source": [
        "Load Models: Human Detectors, Pose Estimators, Action Classifers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03596311",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-04-27T14:33:28.752428Z",
          "start_time": "2022-04-27T14:33:27.930825Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03596311",
        "outputId": "9829b204-cbe1-491b-88e8-6cb973997c36"
      },
      "outputs": [],
      "source": [
        "# the 'slow' relates to the large size of the models and their low inference speed. \n",
        "# We provide the chance to utilize heavy models 'slow' or light models 'fast' in the main loop.\n",
        "detector_slow = model_zoo.get_model('faster_rcnn_resnet50_v1b_voc', pretrained=True)\n",
        "pose_net_slow = model_zoo.get_model('alpha_pose_resnet101_v1b_coco', pretrained=True)\n",
        "detector_slow.reset_class([\"person\"], reuse_weights=['person'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0adf0b17",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-04-27T14:33:29.420463Z",
          "start_time": "2022-04-27T14:33:28.754413Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0adf0b17",
        "outputId": "8ff5db16-7e84-4bfe-9305-6a107b760b35"
      },
      "outputs": [],
      "source": [
        "ctx = mx.cpu() # we run on a CPU\n",
        "#ctx = mx.gpu()\n",
        "\n",
        "# 'fast' relates to the small size of the models and their fast inference speed.\n",
        "# We provide the chance to utilize heavy models 'slow' or light models 'fast' in the main loop.\n",
        "\n",
        "detector_name = \"ssd_512_mobilenet1.0_coco\"\n",
        "detector_fast = get_model(detector_name, pretrained=True, ctx=ctx)\n",
        "\n",
        "detector_fast.reset_class(classes=['person'], reuse_weights={'person':'person'}) #removing all classes but 'person' - makes inference faster.\n",
        "detector_fast.hybridize()\n",
        "\n",
        "pose_net_fast = get_model('simple_pose_resnet18_v1b', pretrained='ccd24037', ctx=ctx)\n",
        "pose_net_fast.hybridize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b629291f",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-04-27T14:33:29.438480Z",
          "start_time": "2022-04-27T14:33:29.422483Z"
        },
        "id": "b629291f"
      },
      "outputs": [],
      "source": [
        "## Alternative Approach - Insert own path for weights\n",
        "#action_classifier_net = ActionClassifier('/content/gdrive/MyDrive/Colab_Notebooks/DLAV/Project/tf-pose-estimation-master/action_recognition_model/action_recognition.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22492dee",
      "metadata": {
        "id": "22492dee"
      },
      "source": [
        "Run Pipeline on Webcam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81149291",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-04-27T14:46:43.183025Z",
          "start_time": "2022-04-27T14:46:19.380689Z"
        },
        "id": "81149291"
      },
      "outputs": [],
      "source": [
        "axes = None\n",
        "Fast = True # choose to utilze the lightweight, high-inference-speed detection and pose-estimation models (was always kept as 'True' during Demo and Testing)\n",
        "\n",
        "trigger = 'Power to the People' # Type of Pose we want as our trigger\n",
        "\n",
        "cap = cv2.VideoCapture(0)\n",
        "time.sleep(1)  ### letting the camera autofocus\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    frame = mx.nd.array(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)).astype('uint8') # convert to an mxnet array\n",
        "    t1 = time.time()\n",
        "\n",
        "    if Fast == True: # utilize lightweight models for faster inference\n",
        "        \n",
        "        x, frame = gcv.data.transforms.presets.ssd.transform_test(frame, short=512, max_size=350) # resize the frame to size that matches the input size required by our detector model\n",
        "        x = x.as_in_context(ctx) # use on CPU\n",
        "        class_IDs, scores, bounding_boxs = detector_fast(x) # returns bounding boxes of detected people\n",
        "\n",
        "        pose_input, upscale_bbox = detector_to_simple_pose(frame, class_IDs, scores, bounding_boxs,\n",
        "                                                           output_shape=(128, 96), ctx=ctx) # resizes bboxes to a (128,96) size, the required input size for our pose-estimation model\n",
        "                                                                                            # upscale_bbox contains slightly up-scaled bboxes \n",
        "        if len(upscale_bbox) > 0:\n",
        "            \n",
        "            predicted_heatmap = pose_net_fast(pose_input) # Estimates pose heatmap for every (128,96) bbox input\n",
        "            pred_coords, confidence = heatmap_to_coord(predicted_heatmap, upscale_bbox) # converts heatmap to a list of keypoints\n",
        "\n",
        "            img = cv_plot_keypoints(frame, pred_coords, confidence, class_IDs, bounding_boxs, scores,\n",
        "                        box_thresh=1, keypoint_thresh=0.2) # plotting / rendereing the keypoints\n",
        "            \n",
        "            for idx, skeleton in enumerate(pred_coords): # looping over every estimated skeleton to check for the 'trigger'\n",
        "                \n",
        "                action_label = classifyPose(skeleton.asnumpy()) # calculates the angle between target keypoints and classifies the pose as a 'trigger' pose or not\n",
        "\n",
        "                if action_label == trigger and scores[0, idx]>0.8:\n",
        "                    # renders / plots and returns the bbox of the trigger person only\n",
        "                    bbox_trigger = bounding_boxs[0].asnumpy()\n",
        "                    top_left = (int(bbox_trigger[idx,0]),int(bbox_trigger[idx,1]))\n",
        "                    bottom_right = (int(bbox_trigger[idx,2]),int(bbox_trigger[idx,3]))\n",
        "                    img = cv2.rectangle(img, top_left, bottom_right, (0,255,0), 2)\n",
        "                    cv2.putText(img, 'Triggered', (top_left[0], top_left[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (36,255,12), 2)\n",
        "                \n",
        "    \n",
        "    else: # utilize heavy models for higher accuracy - lower inference speed (same code as above)\n",
        "        \n",
        "        x, frame = gcv.data.transforms.presets.rcnn.transform_test(frame)\n",
        "\n",
        "        x = x.as_in_context(ctx)\n",
        "        class_IDs, scores, bounding_boxs = detector_slow(x)\n",
        "        pose_input, upscale_bbox = detector_to_alpha_pose(frame, class_IDs, scores, bounding_boxs, ctx=ctx) # output_shape=(128, 96)\n",
        "        \n",
        "        if len(upscale_bbox) > 0:\n",
        "            \n",
        "            predicted_heatmap = pose_net_slow(pose_input)\n",
        "            pred_coords, confidence = heatmap_to_coord_alpha_pose(predicted_heatmap, upscale_bbox)\n",
        "\n",
        "            img = cv_plot_keypoints(frame, pred_coords, confidence, class_IDs, bounding_boxs, scores,\n",
        "                                    box_thresh=0.5, keypoint_thresh=0.2)\n",
        "            \n",
        "            for idx, skeleton in enumerate(pred_coords):\n",
        "                action_label = classifyPose(skeleton.asnumpy()) \n",
        "                if action_label == trigger:\n",
        "\n",
        "                    bbox_trigger = bounding_boxs[0].asnumpy()\n",
        "                    top_left = (int(bbox_trigger[idx,0]),int(bbox_trigger[idx,1]))\n",
        "                    bottom_right = (int(bbox_trigger[idx,2]),int(bbox_trigger[idx,3]))\n",
        "                    img = cv2.rectangle(img, top_left, bottom_right, (0,255,0), 2)\n",
        "                    cv2.putText(img, 'Triggered', (top_left[0], top_left[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (36,255,12), 2)\n",
        "\n",
        "            \n",
        "    cv_plot_image(img) # shows a new render every frame\n",
        "    \n",
        "    k = cv2.waitKey(33)\n",
        "    if k==27:      # Esc key to stop\n",
        "        break\n",
        "    \n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "614e18c8",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-04-27T14:39:09.446011Z",
          "start_time": "2022-04-27T14:39:09.423714Z"
        },
        "id": "614e18c8"
      },
      "outputs": [],
      "source": [
        "cv2.destroyAllWindows()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "MileStone1.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "8aa5943edb16fb267cecf20eec2053f7499d49b23c76e58b423d3578d600dfb2"
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('dlav_final')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
